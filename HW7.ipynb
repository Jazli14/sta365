{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Jazli Muhammad Khairi Leong\n",
        "# Student #: 1007793595\n",
        "# UTORid: muham283"
      ],
      "metadata": {
        "id": "lLpWpF9NHqC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Describe how the posterior predictive distribution is created for mixture models\n",
        "\n",
        "1. Randomly select a draw from the posterior distribution.\n",
        "\n",
        "2. Extract mu, sigma, and weights from the draw.\n",
        "\n",
        "3. Define a mixture model using these parameters.\n",
        "\n",
        "4. Plot the mixture model to represent the posterior predictive distribution.\n",
        "\n",
        "5. Repeat steps 1-4 for multiple draws to get several distributions.\n",
        "\n",
        "6. Now average the distributions to find the mean of the posterior predictive distribution."
      ],
      "metadata": {
        "id": "I-F9d0Q1H1u1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Describe how the posterior predictive distribution is created in general\n",
        "For a general case, randomly sample parameters from the posterior distribution using MCMC methods.Generate new data by simulating outcomes based on each set of sampled parameter values. Collect all the simulated data generated from different parameter sets. Consider the gathered simulated data as the representation of the posterior predictive distribution, reflecting the model's uncertainty and potential outcomes. As pratical Bayesians, we should employ this integral with MCMC $\\int p(y | \\theta)p(\\theta | x, y)d\\theta$"
      ],
      "metadata": {
        "id": "TYKY_WNDH5gs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Have a glance through this and then describe how, if you were doing a regression of $y$ on $X$ but $X$ had some missing values, you could perform a Bayesian analysis without throwing away the rows with missing values in $X$\n",
        "\n",
        "### **Hint: latent variables $v$ indicating the subpopulation are competely missing values that we simply treat as paramters to be inferred though posterior analysis... the same sort of thing can be done with missing values in data that need to be imputed... we should just be careful about the MCAR assumption...**\n",
        "\n",
        "Missing values in $X$, latent variables can represent the subpopulation with missing values, denoted as $v$ in Bayesian regression analysis. These latent variables are treated as parameters to be inferred through posterior analysis. By incorporating $v$ into the model, we can account for the missingness mechanism without discarding observations with missing $X$ values. Through Bayesian inference, we estimate the posterior distribution of model parameters, including coefficients and the missingness mechanism represented by $v$. However, it's crucial to be cautious about the Missing Completely at Random (MCAR) assumption, as deviations from this assumption can affect the validity of inferences. By leveraging latent variables and Bayesian methods, we can perform regression analysis robustly even with missing data, ensuring that valuable information from all available observations is utilized in the analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wS7xPhQEH8ON"
      }
    }
  ]
}
